{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skrowling/Leaf-Disease/blob/master/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BQFquT6UF1Ja",
        "colab_type": "code",
        "outputId": "6df1322a-0dbe-47cd-a760-6059b09e2723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/skrowling/Leaf-Disease-Detection.git\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Leaf-Disease-Detection'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/26)   \u001b[K\rremote: Counting objects:   7% (2/26)   \u001b[K\rremote: Counting objects:  11% (3/26)   \u001b[K\rremote: Counting objects:  15% (4/26)   \u001b[K\rremote: Counting objects:  19% (5/26)   \u001b[K\rremote: Counting objects:  23% (6/26)   \u001b[K\rremote: Counting objects:  26% (7/26)   \u001b[K\rremote: Counting objects:  30% (8/26)   \u001b[K\rremote: Counting objects:  34% (9/26)   \u001b[K\rremote: Counting objects:  38% (10/26)   \u001b[K\rremote: Counting objects:  42% (11/26)   \u001b[K\rremote: Counting objects:  46% (12/26)   \u001b[K\rremote: Counting objects:  50% (13/26)   \u001b[K\rremote: Counting objects:  53% (14/26)   \u001b[K\rremote: Counting objects:  57% (15/26)   \u001b[K\rremote: Counting objects:  61% (16/26)   \u001b[K\rremote: Counting objects:  65% (17/26)   \u001b[K\rremote: Counting objects:  69% (18/26)   \u001b[K\rremote: Counting objects:  73% (19/26)   \u001b[K\rremote: Counting objects:  76% (20/26)   \u001b[K\rremote: Counting objects:  80% (21/26)   \u001b[K\rremote: Counting objects:  84% (22/26)   \u001b[K\rremote: Counting objects:  88% (23/26)   \u001b[K\rremote: Counting objects:  92% (24/26)   \u001b[K\rremote: Counting objects:  96% (25/26)   \u001b[K\rremote: Counting objects: 100% (26/26)   \u001b[K\rremote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/26)   \u001b[K\rremote: Compressing objects:   7% (2/26)   \u001b[K\rremote: Compressing objects:  11% (3/26)   \u001b[K\rremote: Compressing objects:  15% (4/26)   \u001b[K\rremote: Compressing objects:  19% (5/26)   \u001b[K\rremote: Compressing objects:  23% (6/26)   \u001b[K\rremote: Compressing objects:  26% (7/26)   \u001b[K\rremote: Compressing objects:  30% (8/26)   \u001b[K\rremote: Compressing objects:  34% (9/26)   \u001b[K\rremote: Compressing objects:  38% (10/26)   \u001b[K\rremote: Compressing objects:  42% (11/26)   \u001b[K\rremote: Compressing objects:  46% (12/26)   \u001b[K\rremote: Compressing objects:  50% (13/26)   \u001b[K\rremote: Compressing objects:  53% (14/26)   \u001b[K\rremote: Compressing objects:  57% (15/26)   \u001b[K\rremote: Compressing objects:  61% (16/26)   \u001b[K\rremote: Compressing objects:  65% (17/26)   \u001b[K\rremote: Compressing objects:  69% (18/26)   \u001b[K\rremote: Compressing objects:  73% (19/26)   \u001b[K\rremote: Compressing objects:  76% (20/26)   \u001b[K\rremote: Compressing objects:  80% (21/26)   \u001b[K\rremote: Compressing objects:  84% (22/26)   \u001b[K\rremote: Compressing objects:  88% (23/26)   \u001b[K\rremote: Compressing objects:  92% (24/26)   \u001b[K\rremote: Compressing objects:  96% (25/26)   \u001b[K\rremote: Compressing objects: 100% (26/26)   \u001b[K\rremote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "Unpacking objects:   3% (1/26)   \rUnpacking objects:   7% (2/26)   \rUnpacking objects:  11% (3/26)   \rUnpacking objects:  15% (4/26)   \rUnpacking objects:  19% (5/26)   \rUnpacking objects:  23% (6/26)   \rUnpacking objects:  26% (7/26)   \rUnpacking objects:  30% (8/26)   \rUnpacking objects:  34% (9/26)   \rUnpacking objects:  38% (10/26)   \rremote: Total 26 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  42% (11/26)   \rUnpacking objects:  46% (12/26)   \rUnpacking objects:  50% (13/26)   \rUnpacking objects:  53% (14/26)   \rUnpacking objects:  57% (15/26)   \rUnpacking objects:  61% (16/26)   \rUnpacking objects:  65% (17/26)   \rUnpacking objects:  69% (18/26)   \rUnpacking objects:  73% (19/26)   \rUnpacking objects:  76% (20/26)   \rUnpacking objects:  80% (21/26)   \rUnpacking objects:  84% (22/26)   \rUnpacking objects:  88% (23/26)   \rUnpacking objects:  92% (24/26)   \rUnpacking objects:  96% (25/26)   \rUnpacking objects: 100% (26/26)   \rUnpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j3pyR65CGCEs",
        "colab_type": "code",
        "outputId": "971483e2-fa5c-4602-a179-ba35d8be0580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FYOR6vc4GIdt",
        "colab_type": "code",
        "outputId": "eb0c5cdb-c9e7-42c4-8c09-0afb44193f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd Leaf-Disease-Detection\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Leaf-Disease-Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gc_7NiJEGOnS",
        "colab_type": "code",
        "outputId": "e2fe2d6a-2ef8-4142-a6e8-6e210c3b964f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "!python script.py\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Enter Image File Name:\n",
            "late_blight (5).JPG\n",
            "Model loaded successfully.\n",
            "Status: Unhealthy.\n",
            "Disease: Lateblight.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LZHjlkJGGSyp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lu2bgN_CNRmb",
        "colab_type": "code",
        "outputId": "1578effb-77b4-420e-e53f-769e667e22fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd train\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Leaf-Disease-Detection/train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dj1glCP8NY-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hQxNTlyNdBI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CicI85_vNoux",
        "colab_type": "code",
        "outputId": "60c1a1ce-2193-47b7-f899-9f87d68d0d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd ..\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Leaf-Disease-Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kKyphJSjN9CU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFSNsmNWPg6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNbg4qlYPkIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ma_QtG_zPogl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd ..\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_nJI1h0AQNAk",
        "colab_type": "code",
        "outputId": "f1ca1d47-ccf8-448f-f56d-bd50298cda87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8588
        }
      },
      "cell_type": "code",
      "source": [
        "!python neural_network.py\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "100% 2420/2420 [00:02<00:00, 941.86it/s]\n",
            "---------------------------------\n",
            "Run id: leafdiseasedetection-0.001-2conv-basic.model\n",
            "Log directory: log/\n",
            "---------------------------------\n",
            "Training samples: 1920\n",
            "Validation samples: 500\n",
            "--\n",
            "Training Step: 1  | time: 1.098s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 0064/1920\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.38164\u001b[0m\u001b[0m | time: 1.118s\n",
            "| Adam | epoch: 001 | loss: 1.38164 - acc: 0.2391 -- iter: 0128/1920\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m3.59060\u001b[0m\u001b[0m | time: 1.138s\n",
            "| Adam | epoch: 001 | loss: 3.59060 - acc: 0.2352 -- iter: 0192/1920\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.27215\u001b[0m\u001b[0m | time: 1.157s\n",
            "| Adam | epoch: 001 | loss: 2.27215 - acc: 0.2932 -- iter: 0256/1920\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.85421\u001b[0m\u001b[0m | time: 1.176s\n",
            "| Adam | epoch: 001 | loss: 1.85421 - acc: 0.2308 -- iter: 0320/1920\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.54254\u001b[0m\u001b[0m | time: 1.194s\n",
            "| Adam | epoch: 001 | loss: 1.54254 - acc: 0.2733 -- iter: 0384/1920\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.46836\u001b[0m\u001b[0m | time: 1.212s\n",
            "| Adam | epoch: 001 | loss: 1.46836 - acc: 0.2406 -- iter: 0448/1920\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.43330\u001b[0m\u001b[0m | time: 1.230s\n",
            "| Adam | epoch: 001 | loss: 1.43330 - acc: 0.2371 -- iter: 0512/1920\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.39305\u001b[0m\u001b[0m | time: 1.249s\n",
            "| Adam | epoch: 001 | loss: 1.39305 - acc: 0.2770 -- iter: 0576/1920\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.45629\u001b[0m\u001b[0m | time: 1.268s\n",
            "| Adam | epoch: 001 | loss: 1.45629 - acc: 0.2401 -- iter: 0640/1920\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.42259\u001b[0m\u001b[0m | time: 1.286s\n",
            "| Adam | epoch: 001 | loss: 1.42259 - acc: 0.2522 -- iter: 0704/1920\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.40727\u001b[0m\u001b[0m | time: 1.304s\n",
            "| Adam | epoch: 001 | loss: 1.40727 - acc: 0.2301 -- iter: 0768/1920\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.38975\u001b[0m\u001b[0m | time: 1.324s\n",
            "| Adam | epoch: 001 | loss: 1.38975 - acc: 0.2453 -- iter: 0832/1920\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.38743\u001b[0m\u001b[0m | time: 1.349s\n",
            "| Adam | epoch: 001 | loss: 1.38743 - acc: 0.2600 -- iter: 0896/1920\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.37132\u001b[0m\u001b[0m | time: 1.366s\n",
            "| Adam | epoch: 001 | loss: 1.37132 - acc: 0.2867 -- iter: 0960/1920\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.38316\u001b[0m\u001b[0m | time: 1.382s\n",
            "| Adam | epoch: 001 | loss: 1.38316 - acc: 0.2553 -- iter: 1024/1920\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.36639\u001b[0m\u001b[0m | time: 1.400s\n",
            "| Adam | epoch: 001 | loss: 1.36639 - acc: 0.3040 -- iter: 1088/1920\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.34110\u001b[0m\u001b[0m | time: 1.417s\n",
            "| Adam | epoch: 001 | loss: 1.34110 - acc: 0.3935 -- iter: 1152/1920\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.32914\u001b[0m\u001b[0m | time: 1.434s\n",
            "| Adam | epoch: 001 | loss: 1.32914 - acc: 0.4134 -- iter: 1216/1920\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.30316\u001b[0m\u001b[0m | time: 1.451s\n",
            "| Adam | epoch: 001 | loss: 1.30316 - acc: 0.3910 -- iter: 1280/1920\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.33944\u001b[0m\u001b[0m | time: 1.468s\n",
            "| Adam | epoch: 001 | loss: 1.33944 - acc: 0.3569 -- iter: 1344/1920\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.32634\u001b[0m\u001b[0m | time: 1.485s\n",
            "| Adam | epoch: 001 | loss: 1.32634 - acc: 0.3577 -- iter: 1408/1920\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.29592\u001b[0m\u001b[0m | time: 1.502s\n",
            "| Adam | epoch: 001 | loss: 1.29592 - acc: 0.3763 -- iter: 1472/1920\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.27926\u001b[0m\u001b[0m | time: 1.525s\n",
            "| Adam | epoch: 001 | loss: 1.27926 - acc: 0.3847 -- iter: 1536/1920\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.23670\u001b[0m\u001b[0m | time: 1.545s\n",
            "| Adam | epoch: 001 | loss: 1.23670 - acc: 0.4588 -- iter: 1600/1920\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.23845\u001b[0m\u001b[0m | time: 1.563s\n",
            "| Adam | epoch: 001 | loss: 1.23845 - acc: 0.4614 -- iter: 1664/1920\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.24540\u001b[0m\u001b[0m | time: 1.581s\n",
            "| Adam | epoch: 001 | loss: 1.24540 - acc: 0.4392 -- iter: 1728/1920\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.21459\u001b[0m\u001b[0m | time: 1.598s\n",
            "| Adam | epoch: 001 | loss: 1.21459 - acc: 0.4505 -- iter: 1792/1920\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.16338\u001b[0m\u001b[0m | time: 1.615s\n",
            "| Adam | epoch: 001 | loss: 1.16338 - acc: 0.4891 -- iter: 1856/1920\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.28140\u001b[0m\u001b[0m | time: 2.670s\n",
            "| Adam | epoch: 001 | loss: 1.28140 - acc: 0.4436 | val_loss: 0.99877 - val_acc: 0.5700 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.20132\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 002 | loss: 1.20132 - acc: 0.4674 -- iter: 0064/1920\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.14414\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 002 | loss: 1.14414 - acc: 0.4818 -- iter: 0128/1920\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.10265\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 002 | loss: 1.10265 - acc: 0.5201 -- iter: 0192/1920\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.07517\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 002 | loss: 1.07517 - acc: 0.5459 -- iter: 0256/1920\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.04968\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 002 | loss: 1.04968 - acc: 0.5592 -- iter: 0320/1920\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.01046\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 002 | loss: 1.01046 - acc: 0.5854 -- iter: 0384/1920\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.97073\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 002 | loss: 0.97073 - acc: 0.5996 -- iter: 0448/1920\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.95164\u001b[0m\u001b[0m | time: 0.137s\n",
            "| Adam | epoch: 002 | loss: 0.95164 - acc: 0.6046 -- iter: 0512/1920\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.95661\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 002 | loss: 0.95661 - acc: 0.5816 -- iter: 0576/1920\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.92768\u001b[0m\u001b[0m | time: 1.177s\n",
            "| Adam | epoch: 002 | loss: 0.92768 - acc: 0.5838 | val_loss: 0.83923 - val_acc: 0.6160 -- iter: 0640/1920\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.88856\u001b[0m\u001b[0m | time: 1.195s\n",
            "| Adam | epoch: 002 | loss: 0.88856 - acc: 0.6230 -- iter: 0704/1920\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.86285\u001b[0m\u001b[0m | time: 1.212s\n",
            "| Adam | epoch: 002 | loss: 0.86285 - acc: 0.6374 -- iter: 0768/1920\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.85778\u001b[0m\u001b[0m | time: 1.229s\n",
            "| Adam | epoch: 002 | loss: 0.85778 - acc: 0.6490 -- iter: 0832/1920\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.84132\u001b[0m\u001b[0m | time: 1.246s\n",
            "| Adam | epoch: 002 | loss: 0.84132 - acc: 0.6584 -- iter: 0896/1920\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.78156\u001b[0m\u001b[0m | time: 1.263s\n",
            "| Adam | epoch: 002 | loss: 0.78156 - acc: 0.6819 -- iter: 0960/1920\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.79503\u001b[0m\u001b[0m | time: 1.280s\n",
            "| Adam | epoch: 002 | loss: 0.79503 - acc: 0.6646 -- iter: 1024/1920\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.76958\u001b[0m\u001b[0m | time: 1.298s\n",
            "| Adam | epoch: 002 | loss: 0.76958 - acc: 0.6786 -- iter: 1088/1920\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.77542\u001b[0m\u001b[0m | time: 1.315s\n",
            "| Adam | epoch: 002 | loss: 0.77542 - acc: 0.6775 -- iter: 1152/1920\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.78359\u001b[0m\u001b[0m | time: 1.332s\n",
            "| Adam | epoch: 002 | loss: 0.78359 - acc: 0.6667 -- iter: 1216/1920\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.77097\u001b[0m\u001b[0m | time: 1.349s\n",
            "| Adam | epoch: 002 | loss: 0.77097 - acc: 0.6845 -- iter: 1280/1920\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.76935\u001b[0m\u001b[0m | time: 1.367s\n",
            "| Adam | epoch: 002 | loss: 0.76935 - acc: 0.6921 -- iter: 1344/1920\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.73580\u001b[0m\u001b[0m | time: 1.384s\n",
            "| Adam | epoch: 002 | loss: 0.73580 - acc: 0.7078 -- iter: 1408/1920\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.72386\u001b[0m\u001b[0m | time: 1.401s\n",
            "| Adam | epoch: 002 | loss: 0.72386 - acc: 0.7117 -- iter: 1472/1920\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.70044\u001b[0m\u001b[0m | time: 1.418s\n",
            "| Adam | epoch: 002 | loss: 0.70044 - acc: 0.7241 -- iter: 1536/1920\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.67185\u001b[0m\u001b[0m | time: 1.435s\n",
            "| Adam | epoch: 002 | loss: 0.67185 - acc: 0.7434 -- iter: 1600/1920\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.65968\u001b[0m\u001b[0m | time: 1.451s\n",
            "| Adam | epoch: 002 | loss: 0.65968 - acc: 0.7444 -- iter: 1664/1920\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.65606\u001b[0m\u001b[0m | time: 1.475s\n",
            "| Adam | epoch: 002 | loss: 0.65606 - acc: 0.7386 -- iter: 1728/1920\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.62708\u001b[0m\u001b[0m | time: 1.492s\n",
            "| Adam | epoch: 002 | loss: 0.62708 - acc: 0.7530 -- iter: 1792/1920\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.60118\u001b[0m\u001b[0m | time: 1.512s\n",
            "| Adam | epoch: 002 | loss: 0.60118 - acc: 0.7631 -- iter: 1856/1920\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.59288\u001b[0m\u001b[0m | time: 2.532s\n",
            "| Adam | epoch: 002 | loss: 0.59288 - acc: 0.7613 | val_loss: 1.00814 - val_acc: 0.5880 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.86314\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 003 | loss: 0.86314 - acc: 0.7069 -- iter: 0064/1920\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.86046\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 003 | loss: 0.86046 - acc: 0.6984 -- iter: 0128/1920\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.85266\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 003 | loss: 0.85266 - acc: 0.6891 -- iter: 0192/1920\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.82288\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 003 | loss: 0.82288 - acc: 0.7006 -- iter: 0256/1920\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.81788\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 003 | loss: 0.81788 - acc: 0.6932 -- iter: 0320/1920\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.85311\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 003 | loss: 0.85311 - acc: 0.6678 -- iter: 0384/1920\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.86224\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 003 | loss: 0.86224 - acc: 0.6627 -- iter: 0448/1920\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.84029\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 003 | loss: 0.84029 - acc: 0.6674 -- iter: 0512/1920\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.84751\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 003 | loss: 0.84751 - acc: 0.6588 -- iter: 0576/1920\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.84322\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 003 | loss: 0.84322 - acc: 0.6513 -- iter: 0640/1920\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.80976\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 003 | loss: 0.80976 - acc: 0.6679 -- iter: 0704/1920\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.80157\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 003 | loss: 0.80157 - acc: 0.6736 -- iter: 0768/1920\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.80238\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 003 | loss: 0.80238 - acc: 0.6752 -- iter: 0832/1920\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.78557\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 003 | loss: 0.78557 - acc: 0.6817 -- iter: 0896/1920\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.77105\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 003 | loss: 0.77105 - acc: 0.6874 -- iter: 0960/1920\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.73607\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 003 | loss: 0.73607 - acc: 0.6991 -- iter: 1024/1920\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.70664\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 003 | loss: 0.70664 - acc: 0.7095 -- iter: 1088/1920\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.67630\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 003 | loss: 0.67630 - acc: 0.7170 -- iter: 1152/1920\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.65357\u001b[0m\u001b[0m | time: 0.336s\n",
            "| Adam | epoch: 003 | loss: 0.65357 - acc: 0.7220 -- iter: 1216/1920\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.63852\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 003 | loss: 0.63852 - acc: 0.7329 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 1280/1920\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.60975\u001b[0m\u001b[0m | time: 0.375s\n",
            "| Adam | epoch: 003 | loss: 0.60975 - acc: 0.7488 -- iter: 1344/1920\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.59006\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 003 | loss: 0.59006 - acc: 0.7583 -- iter: 1408/1920\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.57486\u001b[0m\u001b[0m | time: 0.410s\n",
            "| Adam | epoch: 003 | loss: 0.57486 - acc: 0.7669 -- iter: 1472/1920\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.55252\u001b[0m\u001b[0m | time: 0.427s\n",
            "| Adam | epoch: 003 | loss: 0.55252 - acc: 0.7761 -- iter: 1536/1920\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.54245\u001b[0m\u001b[0m | time: 0.452s\n",
            "| Adam | epoch: 003 | loss: 0.54245 - acc: 0.7797 -- iter: 1600/1920\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.53102\u001b[0m\u001b[0m | time: 0.469s\n",
            "| Adam | epoch: 003 | loss: 0.53102 - acc: 0.7877 -- iter: 1664/1920\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.50896\u001b[0m\u001b[0m | time: 0.486s\n",
            "| Adam | epoch: 003 | loss: 0.50896 - acc: 0.7996 -- iter: 1728/1920\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.50283\u001b[0m\u001b[0m | time: 0.503s\n",
            "| Adam | epoch: 003 | loss: 0.50283 - acc: 0.8040 -- iter: 1792/1920\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.51500\u001b[0m\u001b[0m | time: 0.520s\n",
            "| Adam | epoch: 003 | loss: 0.51500 - acc: 0.7955 -- iter: 1856/1920\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.53039\u001b[0m\u001b[0m | time: 1.543s\n",
            "| Adam | epoch: 003 | loss: 0.53039 - acc: 0.7893 | val_loss: 0.73421 - val_acc: 0.7400 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.52044\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 004 | loss: 0.52044 - acc: 0.7964 -- iter: 0064/1920\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.70030\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 004 | loss: 0.70030 - acc: 0.7683 -- iter: 0128/1920\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.74478\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 004 | loss: 0.74478 - acc: 0.7555 -- iter: 0192/1920\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.74183\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 004 | loss: 0.74183 - acc: 0.7518 -- iter: 0256/1920\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.70951\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 004 | loss: 0.70951 - acc: 0.7595 -- iter: 0320/1920\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.69136\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 004 | loss: 0.69136 - acc: 0.7616 -- iter: 0384/1920\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.65466\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 004 | loss: 0.65466 - acc: 0.7745 -- iter: 0448/1920\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.61902\u001b[0m\u001b[0m | time: 0.135s\n",
            "| Adam | epoch: 004 | loss: 0.61902 - acc: 0.7862 -- iter: 0512/1920\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.58350\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 004 | loss: 0.58350 - acc: 0.7997 -- iter: 0576/1920\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.55278\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 004 | loss: 0.55278 - acc: 0.8135 -- iter: 0640/1920\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.51664\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 004 | loss: 0.51664 - acc: 0.8259 -- iter: 0704/1920\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.49027\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 004 | loss: 0.49027 - acc: 0.8324 -- iter: 0768/1920\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.48048\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 004 | loss: 0.48048 - acc: 0.8351 -- iter: 0832/1920\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.47428\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 004 | loss: 0.47428 - acc: 0.8344 -- iter: 0896/1920\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.46416\u001b[0m\u001b[0m | time: 0.253s\n",
            "| Adam | epoch: 004 | loss: 0.46416 - acc: 0.8369 -- iter: 0960/1920\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.46206\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 004 | loss: 0.46206 - acc: 0.8376 -- iter: 1024/1920\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.46862\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 004 | loss: 0.46862 - acc: 0.8319 -- iter: 1088/1920\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.45896\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 004 | loss: 0.45896 - acc: 0.8378 -- iter: 1152/1920\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.44331\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 004 | loss: 0.44331 - acc: 0.8431 -- iter: 1216/1920\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.42089\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 004 | loss: 0.42089 - acc: 0.8478 -- iter: 1280/1920\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.40356\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 004 | loss: 0.40356 - acc: 0.8537 -- iter: 1344/1920\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.39991\u001b[0m\u001b[0m | time: 0.374s\n",
            "| Adam | epoch: 004 | loss: 0.39991 - acc: 0.8542 -- iter: 1408/1920\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.41381\u001b[0m\u001b[0m | time: 0.390s\n",
            "| Adam | epoch: 004 | loss: 0.41381 - acc: 0.8485 -- iter: 1472/1920\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.38849\u001b[0m\u001b[0m | time: 0.406s\n",
            "| Adam | epoch: 004 | loss: 0.38849 - acc: 0.8605 -- iter: 1536/1920\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.38172\u001b[0m\u001b[0m | time: 0.422s\n",
            "| Adam | epoch: 004 | loss: 0.38172 - acc: 0.8589 -- iter: 1600/1920\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.36169\u001b[0m\u001b[0m | time: 0.439s\n",
            "| Adam | epoch: 004 | loss: 0.36169 - acc: 0.8683 -- iter: 1664/1920\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.35770\u001b[0m\u001b[0m | time: 0.456s\n",
            "| Adam | epoch: 004 | loss: 0.35770 - acc: 0.8674 -- iter: 1728/1920\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.35523\u001b[0m\u001b[0m | time: 0.471s\n",
            "| Adam | epoch: 004 | loss: 0.35523 - acc: 0.8697 -- iter: 1792/1920\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.35910\u001b[0m\u001b[0m | time: 0.488s\n",
            "| Adam | epoch: 004 | loss: 0.35910 - acc: 0.8687 -- iter: 1856/1920\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.34301\u001b[0m\u001b[0m | time: 1.509s\n",
            "| Adam | epoch: 004 | loss: 0.34301 - acc: 0.8740 | val_loss: 0.32055 - val_acc: 0.8920 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.32439\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 005 | loss: 0.32439 - acc: 0.8788 -- iter: 0064/1920\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.32917\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 005 | loss: 0.32917 - acc: 0.8768 -- iter: 0128/1920\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.32008\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 005 | loss: 0.32008 - acc: 0.8798 -- iter: 0192/1920\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.30682\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 005 | loss: 0.30682 - acc: 0.8871 -- iter: 0256/1920\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.29274\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 005 | loss: 0.29274 - acc: 0.8906 -- iter: 0320/1920\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.30042\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 005 | loss: 0.30042 - acc: 0.8890 -- iter: 0384/1920\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.28350\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 005 | loss: 0.28350 - acc: 0.8939 -- iter: 0448/1920\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.35812\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 005 | loss: 0.35812 - acc: 0.8764 -- iter: 0512/1920\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.48991\u001b[0m\u001b[0m | time: 0.145s\n",
            "| Adam | epoch: 005 | loss: 0.48991 - acc: 0.8309 -- iter: 0576/1920\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.53572\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 005 | loss: 0.53572 - acc: 0.8103 -- iter: 0640/1920\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.52454\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 005 | loss: 0.52454 - acc: 0.8105 -- iter: 0704/1920\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.51738\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 005 | loss: 0.51738 - acc: 0.8076 -- iter: 0768/1920\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.48697\u001b[0m\u001b[0m | time: 0.209s\n",
            "| Adam | epoch: 005 | loss: 0.48697 - acc: 0.8222 -- iter: 0832/1920\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.50699\u001b[0m\u001b[0m | time: 0.225s\n",
            "| Adam | epoch: 005 | loss: 0.50699 - acc: 0.8118 -- iter: 0896/1920\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.48555\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 005 | loss: 0.48555 - acc: 0.8213 -- iter: 0960/1920\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.46655\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 005 | loss: 0.46655 - acc: 0.8266 -- iter: 1024/1920\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.44402\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 005 | loss: 0.44402 - acc: 0.8315 -- iter: 1088/1920\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.44360\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 005 | loss: 0.44360 - acc: 0.8280 -- iter: 1152/1920\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.42585\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 005 | loss: 0.42585 - acc: 0.8374 -- iter: 1216/1920\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.40004\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 005 | loss: 0.40004 - acc: 0.8490 -- iter: 1280/1920\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.39017\u001b[0m\u001b[0m | time: 0.344s\n",
            "| Adam | epoch: 005 | loss: 0.39017 - acc: 0.8578 -- iter: 1344/1920\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.38114\u001b[0m\u001b[0m | time: 0.360s\n",
            "| Adam | epoch: 005 | loss: 0.38114 - acc: 0.8611 -- iter: 1408/1920\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.37268\u001b[0m\u001b[0m | time: 0.375s\n",
            "| Adam | epoch: 005 | loss: 0.37268 - acc: 0.8641 -- iter: 1472/1920\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.35629\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 005 | loss: 0.35629 - acc: 0.8698 -- iter: 1536/1920\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.33791\u001b[0m\u001b[0m | time: 0.406s\n",
            "| Adam | epoch: 005 | loss: 0.33791 - acc: 0.8766 -- iter: 1600/1920\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.33421\u001b[0m\u001b[0m | time: 0.425s\n",
            "| Adam | epoch: 005 | loss: 0.33421 - acc: 0.8796 -- iter: 1664/1920\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.32585\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 005 | loss: 0.32585 - acc: 0.8822 -- iter: 1728/1920\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.30997\u001b[0m\u001b[0m | time: 0.457s\n",
            "| Adam | epoch: 005 | loss: 0.30997 - acc: 0.8862 -- iter: 1792/1920\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.30443\u001b[0m\u001b[0m | time: 0.472s\n",
            "| Adam | epoch: 005 | loss: 0.30443 - acc: 0.8866 -- iter: 1856/1920\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.29703\u001b[0m\u001b[0m | time: 1.492s\n",
            "| Adam | epoch: 005 | loss: 0.29703 - acc: 0.8917 | val_loss: 0.27436 - val_acc: 0.9060 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.29050\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 006 | loss: 0.29050 - acc: 0.8947 -- iter: 0064/1920\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.27476\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 006 | loss: 0.27476 - acc: 0.9006 -- iter: 0128/1920\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.25823\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 006 | loss: 0.25823 - acc: 0.9090 -- iter: 0192/1920\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.28193\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 006 | loss: 0.28193 - acc: 0.9009 -- iter: 0256/1920\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.31942\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 006 | loss: 0.31942 - acc: 0.8842 -- iter: 0320/1920\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.31230\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 006 | loss: 0.31230 - acc: 0.8880 -- iter: 0384/1920\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.29793\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 006 | loss: 0.29793 - acc: 0.8929 -- iter: 0448/1920\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.29075\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 006 | loss: 0.29075 - acc: 0.8927 -- iter: 0512/1920\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.27315\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 006 | loss: 0.27315 - acc: 0.9003 -- iter: 0576/1920\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.28789\u001b[0m\u001b[0m | time: 1.162s\n",
            "| Adam | epoch: 006 | loss: 0.28789 - acc: 0.8978 | val_loss: 0.31223 - val_acc: 0.8960 -- iter: 0640/1920\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.27898\u001b[0m\u001b[0m | time: 1.177s\n",
            "| Adam | epoch: 006 | loss: 0.27898 - acc: 0.8971 -- iter: 0704/1920\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.28456\u001b[0m\u001b[0m | time: 1.192s\n",
            "| Adam | epoch: 006 | loss: 0.28456 - acc: 0.8933 -- iter: 0768/1920\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.28468\u001b[0m\u001b[0m | time: 1.207s\n",
            "| Adam | epoch: 006 | loss: 0.28468 - acc: 0.8930 -- iter: 0832/1920\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.27458\u001b[0m\u001b[0m | time: 1.223s\n",
            "| Adam | epoch: 006 | loss: 0.27458 - acc: 0.8975 -- iter: 0896/1920\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.25114\u001b[0m\u001b[0m | time: 1.238s\n",
            "| Adam | epoch: 006 | loss: 0.25114 - acc: 0.9062 -- iter: 0960/1920\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.25112\u001b[0m\u001b[0m | time: 1.254s\n",
            "| Adam | epoch: 006 | loss: 0.25112 - acc: 0.9077 -- iter: 1024/1920\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.23746\u001b[0m\u001b[0m | time: 1.270s\n",
            "| Adam | epoch: 006 | loss: 0.23746 - acc: 0.9138 -- iter: 1088/1920\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.23785\u001b[0m\u001b[0m | time: 1.285s\n",
            "| Adam | epoch: 006 | loss: 0.23785 - acc: 0.9131 -- iter: 1152/1920\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.23804\u001b[0m\u001b[0m | time: 1.301s\n",
            "| Adam | epoch: 006 | loss: 0.23804 - acc: 0.9108 -- iter: 1216/1920\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.22976\u001b[0m\u001b[0m | time: 1.317s\n",
            "| Adam | epoch: 006 | loss: 0.22976 - acc: 0.9135 -- iter: 1280/1920\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.22054\u001b[0m\u001b[0m | time: 1.332s\n",
            "| Adam | epoch: 006 | loss: 0.22054 - acc: 0.9159 -- iter: 1344/1920\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.21546\u001b[0m\u001b[0m | time: 1.347s\n",
            "| Adam | epoch: 006 | loss: 0.21546 - acc: 0.9149 -- iter: 1408/1920\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.20662\u001b[0m\u001b[0m | time: 1.363s\n",
            "| Adam | epoch: 006 | loss: 0.20662 - acc: 0.9172 -- iter: 1472/1920\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.19775\u001b[0m\u001b[0m | time: 1.379s\n",
            "| Adam | epoch: 006 | loss: 0.19775 - acc: 0.9208 -- iter: 1536/1920\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.22045\u001b[0m\u001b[0m | time: 1.395s\n",
            "| Adam | epoch: 006 | loss: 0.22045 - acc: 0.9115 -- iter: 1600/1920\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.21072\u001b[0m\u001b[0m | time: 1.410s\n",
            "| Adam | epoch: 006 | loss: 0.21072 - acc: 0.9157 -- iter: 1664/1920\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.20180\u001b[0m\u001b[0m | time: 1.434s\n",
            "| Adam | epoch: 006 | loss: 0.20180 - acc: 0.9194 -- iter: 1728/1920\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.18966\u001b[0m\u001b[0m | time: 1.450s\n",
            "| Adam | epoch: 006 | loss: 0.18966 - acc: 0.9244 -- iter: 1792/1920\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.18394\u001b[0m\u001b[0m | time: 1.465s\n",
            "| Adam | epoch: 006 | loss: 0.18394 - acc: 0.9257 -- iter: 1856/1920\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.17531\u001b[0m\u001b[0m | time: 2.484s\n",
            "| Adam | epoch: 006 | loss: 0.17531 - acc: 0.9300 | val_loss: 0.33911 - val_acc: 0.8960 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.16885\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 007 | loss: 0.16885 - acc: 0.9323 -- iter: 0064/1920\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.17817\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 007 | loss: 0.17817 - acc: 0.9313 -- iter: 0128/1920\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.17266\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 007 | loss: 0.17266 - acc: 0.9334 -- iter: 0192/1920\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.17036\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 007 | loss: 0.17036 - acc: 0.9323 -- iter: 0256/1920\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.81024\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 007 | loss: 0.81024 - acc: 0.8656 -- iter: 0320/1920\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.76059\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 007 | loss: 0.76059 - acc: 0.8681 -- iter: 0384/1920\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.71200\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 007 | loss: 0.71200 - acc: 0.8719 -- iter: 0448/1920\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.65661\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 007 | loss: 0.65661 - acc: 0.8816 -- iter: 0512/1920\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.60962\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 007 | loss: 0.60962 - acc: 0.8872 -- iter: 0576/1920\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.57336\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 007 | loss: 0.57336 - acc: 0.8875 -- iter: 0640/1920\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.53791\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 007 | loss: 0.53791 - acc: 0.8910 -- iter: 0704/1920\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.50774\u001b[0m\u001b[0m | time: 0.191s\n",
            "| Adam | epoch: 007 | loss: 0.50774 - acc: 0.8925 -- iter: 0768/1920\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.49332\u001b[0m\u001b[0m | time: 0.206s\n",
            "| Adam | epoch: 007 | loss: 0.49332 - acc: 0.8876 -- iter: 0832/1920\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.49045\u001b[0m\u001b[0m | time: 0.222s\n",
            "| Adam | epoch: 007 | loss: 0.49045 - acc: 0.8832 -- iter: 0896/1920\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.47580\u001b[0m\u001b[0m | time: 0.242s\n",
            "| Adam | epoch: 007 | loss: 0.47580 - acc: 0.8824 -- iter: 0960/1920\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.44248\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 007 | loss: 0.44248 - acc: 0.8895 -- iter: 1024/1920\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.40660\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 007 | loss: 0.40660 - acc: 0.8990 -- iter: 1088/1920\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.38401\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 007 | loss: 0.38401 - acc: 0.9028 -- iter: 1152/1920\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.36212\u001b[0m\u001b[0m | time: 0.304s\n",
            "| Adam | epoch: 007 | loss: 0.36212 - acc: 0.9032 -- iter: 1216/1920\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.34252\u001b[0m\u001b[0m | time: 1.323s\n",
            "| Adam | epoch: 007 | loss: 0.34252 - acc: 0.9097 | val_loss: 0.27802 - val_acc: 0.9160 -- iter: 1280/1920\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.32860\u001b[0m\u001b[0m | time: 1.338s\n",
            "| Adam | epoch: 007 | loss: 0.32860 - acc: 0.9125 -- iter: 1344/1920\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.30471\u001b[0m\u001b[0m | time: 1.355s\n",
            "| Adam | epoch: 007 | loss: 0.30471 - acc: 0.9197 -- iter: 1408/1920\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.30621\u001b[0m\u001b[0m | time: 1.370s\n",
            "| Adam | epoch: 007 | loss: 0.30621 - acc: 0.9183 -- iter: 1472/1920\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.30474\u001b[0m\u001b[0m | time: 1.385s\n",
            "| Adam | epoch: 007 | loss: 0.30474 - acc: 0.9171 -- iter: 1536/1920\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.29659\u001b[0m\u001b[0m | time: 1.401s\n",
            "| Adam | epoch: 007 | loss: 0.29659 - acc: 0.9176 -- iter: 1600/1920\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.27662\u001b[0m\u001b[0m | time: 1.415s\n",
            "| Adam | epoch: 007 | loss: 0.27662 - acc: 0.9227 -- iter: 1664/1920\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.26702\u001b[0m\u001b[0m | time: 1.431s\n",
            "| Adam | epoch: 007 | loss: 0.26702 - acc: 0.9258 -- iter: 1728/1920\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.25625\u001b[0m\u001b[0m | time: 1.446s\n",
            "| Adam | epoch: 007 | loss: 0.25625 - acc: 0.9269 -- iter: 1792/1920\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.26590\u001b[0m\u001b[0m | time: 1.461s\n",
            "| Adam | epoch: 007 | loss: 0.26590 - acc: 0.9217 -- iter: 1856/1920\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.25029\u001b[0m\u001b[0m | time: 2.481s\n",
            "| Adam | epoch: 007 | loss: 0.25029 - acc: 0.9264 | val_loss: 0.32141 - val_acc: 0.8760 -- iter: 1920/1920\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.24176\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 008 | loss: 0.24176 - acc: 0.9291 -- iter: 0064/1920\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.23918\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 008 | loss: 0.23918 - acc: 0.9284 -- iter: 0128/1920\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.23789\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 008 | loss: 0.23789 - acc: 0.9246 -- iter: 0192/1920\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.22744\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 008 | loss: 0.22744 - acc: 0.9275 -- iter: 0256/1920\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.22265\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 008 | loss: 0.22265 - acc: 0.9269 -- iter: 0320/1920\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.22157\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 008 | loss: 0.22157 - acc: 0.9295 -- iter: 0384/1920\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.20387\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 008 | loss: 0.20387 - acc: 0.9350 -- iter: 0448/1920\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.19157\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 008 | loss: 0.19157 - acc: 0.9384 -- iter: 0512/1920\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.18856\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 008 | loss: 0.18856 - acc: 0.9414 -- iter: 0576/1920\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.18733\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 008 | loss: 0.18733 - acc: 0.9379 -- iter: 0640/1920\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.17333\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 008 | loss: 0.17333 - acc: 0.9441 -- iter: 0704/1920\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.16514\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 008 | loss: 0.16514 - acc: 0.9466 -- iter: 0768/1920\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.15493\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 008 | loss: 0.15493 - acc: 0.9504 -- iter: 0832/1920\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.14245\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 008 | loss: 0.14245 - acc: 0.9553 -- iter: 0896/1920\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.13525\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 008 | loss: 0.13525 - acc: 0.9598 -- iter: 0960/1920\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.12553\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 008 | loss: 0.12553 - acc: 0.9638 -- iter: 1024/1920\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.11841\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 008 | loss: 0.11841 - acc: 0.9643 -- iter: 1088/1920\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.11959\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 008 | loss: 0.11959 - acc: 0.9616 -- iter: 1152/1920\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.11870\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 008 | loss: 0.11870 - acc: 0.9623 -- iter: 1216/1920\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.11549\u001b[0m\u001b[0m | time: 0.318s\n",
            "| Adam | epoch: 008 | loss: 0.11549 - acc: 0.9630 -- iter: 1280/1920\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.12239\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 008 | loss: 0.12239 - acc: 0.9620 -- iter: 1344/1920\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.11972\u001b[0m\u001b[0m | time: 0.349s\n",
            "| Adam | epoch: 008 | loss: 0.11972 - acc: 0.9642 -- iter: 1408/1920\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.11695\u001b[0m\u001b[0m | time: 0.365s\n",
            "| Adam | epoch: 008 | loss: 0.11695 - acc: 0.9647 -- iter: 1472/1920\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.10809\u001b[0m\u001b[0m | time: 0.380s\n",
            "| Adam | epoch: 008 | loss: 0.10809 - acc: 0.9667 -- iter: 1536/1920\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.10100\u001b[0m\u001b[0m | time: 0.396s\n",
            "| Adam | epoch: 008 | loss: 0.10100 - acc: 0.9684 -- iter: 1600/1920\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.10629\u001b[0m\u001b[0m | time: 0.413s\n",
            "| Adam | epoch: 008 | loss: 0.10629 - acc: 0.9669 -- iter: 1664/1920\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.12140\u001b[0m\u001b[0m | time: 0.428s\n",
            "| Adam | epoch: 008 | loss: 0.12140 - acc: 0.9624 -- iter: 1728/1920\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.11540\u001b[0m\u001b[0m | time: 0.444s\n",
            "| Adam | epoch: 008 | loss: 0.11540 - acc: 0.9630 -- iter: 1792/1920\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.11537\u001b[0m\u001b[0m | time: 0.459s\n",
            "| Adam | epoch: 008 | loss: 0.11537 - acc: 0.9620 -- iter: 1856/1920\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.11710\u001b[0m\u001b[0m | time: 1.485s\n",
            "| Adam | epoch: 008 | loss: 0.11710 - acc: 0.9627 | val_loss: 0.20693 - val_acc: 0.9320 -- iter: 1920/1920\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z_C7KhnVQTUP",
        "colab_type": "code",
        "outputId": "c210ef68-1aea-475b-a0c9-4342f8a84764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_process.py  neural_network.py  script.py  test2.jpg  test4.jpg\n",
            "\u001b[0m\u001b[01;34mlog\u001b[0m/              README.md          test1.jpg  test3.jpg  \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "icaSp0_nZ2cp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rm -r log\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hkvsItqaIEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python image_process.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mYhogXxnaN7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rm -r Leaf-Disease-Detection/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyWh_GYpn7nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}